{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom template to guide llm model\n",
    "custom_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "\n",
    "CUSTOM_QUESTION_PROMPT = PromptTemplate.from_template(custom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting text from pdf\n",
    "def get_pdf_text(docs):\n",
    "    text=\"\"\n",
    "    for pdf in docs:\n",
    "        pdf_reader=PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text+=page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to chunks\n",
    "def get_chunks(raw_text):\n",
    "    text_splitter=CharacterTextSplitter(separator=\"\\n\",\n",
    "                                        chunk_size=100,\n",
    "                                        chunk_overlap=10,\n",
    "                                        length_function=len)   \n",
    "    chunks=text_splitter.split_text(raw_text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all-MiniLm embeddings model and faiss to get vectorstore\n",
    "def get_vectorstore(chunks):\n",
    "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                                     model_kwargs={'device':'cpu'})\n",
    "    vectorstore=faiss.FAISS.from_texts(texts=chunks,embedding=embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating conversation chain  \n",
    "def get_conversationchain(vectorstore):\n",
    "    llm=ChatOpenAI(temperature=0.2)\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key='chat_history', \n",
    "        return_messages=True,\n",
    "        output_key='answer'\n",
    "    ) # using conversation buffer memory to hold past information\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "                                llm=llm,\n",
    "                                retriever=vectorstore.as_retriever(),\n",
    "                                condense_question_prompt=CUSTOM_QUESTION_PROMPT,\n",
    "                                memory=memory)\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is GPT-4o?\n",
      "GPT-4o is OpenAI’s latest flagship model that can reason across audio, vision, and text in real time. OpenAI claims it's a step “towards much more natural\n",
      "human-computer interaction.” \n",
      "How is GPT-4o different than other versions of ChatGPT?\n",
      "GPT-4o differs from other versions of ChatGPT by offering improved performance, including faster response times and better handling of complex queries,\n",
      "while retaining the comprehensive language capabilities of GPT-4o.\n",
      "Is GPT-4o free?\n",
      "Yes, GPT-4o is available to users of the free version of ChatGPT.\n",
      "Is GPT-4o better than GPT-4o?\n",
      "GPT-4o is considered an optimized enhancement of GPT-4o, offering better performance in terms of speed and efficiency. However, the core language\n",
      "capabilities remain consistent with GPT-4o.\n",
      "Is ChatGPT 4o available?\n",
      "Yes, GPT-4o is now available to users. \n",
      "What does GPT-4o do?\n",
      "GPT-4o can assess, summarize, and converse with users via text, visuals, and audio. It also answers your queries by combining model knowledge and\n",
      "information from the internet, helping to provide even better insights.\n",
      "What's new about ChatGPT 4o?GPT-4o is a new, optimized version of OpenAI's GPT-4o, designed to enhance performance and efficiency while maintaining its predecessor's robust language\n",
      "understanding and generation capabilities. It can now converse with you using images, audio, and video.\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path(\"../content/GPT4o.pdf\")\n",
    "\n",
    "text = get_pdf_text([path])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 154, which is longer than the specified 100\n",
      "Created a chunk of size 151, which is longer than the specified 100\n",
      "Created a chunk of size 145, which is longer than the specified 100\n",
      "Created a chunk of size 145, which is longer than the specified 100\n",
      "Created a chunk of size 181, which is longer than the specified 100\n",
      "Created a chunk of size 103, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is GPT-4o?', \"GPT-4o is OpenAI’s latest flagship model that can reason across audio, vision, and text in real time. OpenAI claims it's a step “towards much more natural\", 'human-computer interaction.”\\xa0\\nHow is GPT-4o different than other versions of ChatGPT?', 'GPT-4o differs from other versions of ChatGPT by offering improved performance, including faster response times and better handling of complex queries,', 'while retaining the comprehensive language capabilities of GPT-4o.\\nIs GPT-4o free?', 'Yes, GPT-4o is available to users of the free version of ChatGPT.\\nIs GPT-4o better than GPT-4o?', 'GPT-4o is considered an optimized enhancement of GPT-4o, offering better performance in terms of speed and efficiency. However, the core language', 'capabilities remain consistent with GPT-4o.\\nIs ChatGPT 4o available?', 'Yes, GPT-4o is now available to users.\\xa0\\nWhat does GPT-4o do?', 'GPT-4o can assess, summarize, and converse with users via text, visuals, and audio. It also answers your queries by combining model knowledge and', 'information from the internet, helping to provide even better insights.', \"What's new about ChatGPT 4o?GPT-4o is a new, optimized version of OpenAI's GPT-4o, designed to enhance performance and efficiency while maintaining its predecessor's robust language\", 'understanding and generation capabilities. It can now converse with you using images, audio, and video.']\n"
     ]
    }
   ],
   "source": [
    "# Covnerted into Chunks\n",
    "chunks = get_chunks(text)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x0000024914D08980>\n"
     ]
    }
   ],
   "source": [
    "vectorstore = get_vectorstore(chunks)\n",
    "print(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(output_key='answer', return_messages=True, memory_key='chat_history') combine_docs_chain=StuffDocumentsChain(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['context', 'question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\")), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000024914D0AD20>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000249124D8A40>, root_client=<openai.OpenAI object at 0x0000024965D3BFE0>, root_async_client=<openai.AsyncOpenAI object at 0x0000024914D0AD80>, temperature=0.2, openai_api_key=SecretStr('**********'), openai_proxy='')), document_variable_name='context') question_generator=LLMChain(prompt=PromptTemplate(input_variables=['chat_history', 'question'], template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000024914D0AD20>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x00000249124D8A40>, root_client=<openai.OpenAI object at 0x0000024965D3BFE0>, root_async_client=<openai.AsyncOpenAI object at 0x0000024914D0AD80>, temperature=0.2, openai_api_key=SecretStr('**********'), openai_proxy='')) retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000024914D08980>)\n"
     ]
    }
   ],
   "source": [
    "chain = get_conversationchain(vectorstore)\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain({\n",
    "    \"question\": \"Key features of GPT4o?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Key features of GPT4o?',\n",
       " 'chat_history': [HumanMessage(content='Key features of GPT4o?'),\n",
       "  AIMessage(content='The key features of GPT-4o include improved performance in terms of speed and efficiency compared to its predecessor, GPT-4. It maintains the robust language capabilities of the previous version while offering enhancements in overall functionality.')],\n",
       " 'answer': 'The key features of GPT-4o include improved performance in terms of speed and efficiency compared to its predecessor, GPT-4. It maintains the robust language capabilities of the previous version while offering enhancements in overall functionality.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key features of GPT4o?\n",
      "########################################\n",
      "[HumanMessage(content='Key features of GPT4o?'), AIMessage(content='The key features of GPT-4o include improved performance in terms of speed and efficiency compared to its predecessor, GPT-4. It maintains the robust language capabilities of the previous version while offering enhancements in overall functionality.')]\n",
      "########################################\n",
      "The key features of GPT-4o include improved performance in terms of speed and efficiency compared to its predecessor, GPT-4. It maintains the robust language capabilities of the previous version while offering enhancements in overall functionality.\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "print(response['question'])\n",
    "print(\"#\"*40)\n",
    "print(response['chat_history'])\n",
    "print(\"#\"*40)\n",
    "print(response['answer'])\n",
    "print(\"#\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = chain({\n",
    "    \"question\": \"What is new about in GPT4o?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is new about in GPT4o?\n",
      "########################################\n",
      "[HumanMessage(content='Key features of GPT4o?'), AIMessage(content='The key features of GPT-4o include improved performance in terms of speed and efficiency compared to its predecessor, GPT-4. It maintains the robust language capabilities of the previous version while offering enhancements in overall functionality.'), HumanMessage(content='What is new about in GPT4o?'), AIMessage(content=\"GPT-4o is a new, optimized version of OpenAI's GPT-4, designed to enhance performance and efficiency while maintaining its predecessor's robust language capabilities.\")]\n",
      "########################################\n",
      "GPT-4o is a new, optimized version of OpenAI's GPT-4, designed to enhance performance and efficiency while maintaining its predecessor's robust language capabilities.\n",
      "########################################\n"
     ]
    }
   ],
   "source": [
    "print(response1['question'])\n",
    "print(\"#\"*40)\n",
    "print(response1['chat_history'])\n",
    "print(\"#\"*40)\n",
    "print(response1['answer'])\n",
    "print(\"#\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
